from fastapi import FastAPI, Query, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
from apscheduler.schedulers.background import BackgroundScheduler
from dotenv import load_dotenv
import pandas as pd
import feedparser
import requests
import sqlite3
import time
import os
import json
from bs4 import BeautifulSoup
from datetime import datetime
from fastapi.responses import JSONResponse
from kiwipiepy import Kiwi
import threading
import random

# ===================== [ì´ˆê¸° ì„¤ì •] =====================
load_dotenv()
HF_API_TOKEN = os.getenv("HF_API_TOKEN")
if not HF_API_TOKEN:
    raise RuntimeError("âŒ Hugging Face API í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ í™•ì¸ ìš”ë§.")

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

rss_feeds = {
    "ì „ìì‹ ë¬¸": "https://rss.etnews.com/Section901.xml",
    "í•œê²¨ë ˆ": "https://www.hani.co.kr/rss/",
    "SBS": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01",
    "ë§¤ì¼ê²½ì œ": "https://www.mk.co.kr/rss/40300001/",
    "ì„¸ê³„ì¼ë³´": "https://www.segye.com/Articles/RSSList/segye_recent.xml"
}

kiwi = Kiwi()

# ===================== [DB ë³´ì¥ í•¨ìˆ˜] =====================
def ensure_db():
    db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS news (
            source TEXT,
            title TEXT,
            link TEXT,
            content TEXT,
            summary TEXT,
            date TEXT
        )
    """)
    conn.commit()
    conn.close()
    print("âœ… news í…Œì´ë¸” í™•ì¸ ë˜ëŠ” ìƒì„± ì™„ë£Œ")

# ===================== [ë³¸ë¬¸ í¬ë¡¤ë§ (selector ì ìš©)] =====================
def extract_body(url, source=None):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, timeout=7, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")

        # ì‹ ë¬¸ì‚¬ë³„ selector
        if source == "í•œê²¨ë ˆ":
            article = soup.select_one("div.article-text")
        elif source == "ì „ìì‹ ë¬¸":
            article = soup.select_one("#articleBody")
        elif source == "SBS":
            article = soup.select_one("div.text_area")
        elif source == "ë§¤ì¼ê²½ì œ":
            article = soup.select_one("div#article_body")
        elif source == "ì„¸ê³„ì¼ë³´":
            article = soup.select_one("div#article_txt")
        else:
            article = None

        if article:
            body = article.get_text(separator="\n").strip()
        else:
            paragraphs = soup.find_all("p")
            body = "\n".join(p.get_text().strip() for p in paragraphs if p.get_text().strip())

        lines = [line.strip() for line in body.splitlines() if len(line.strip()) > 20]
        lines = [line for line in lines if "ê¸°ì" not in line and "ë¬´ë‹¨ì „ì¬" not in line]
        clean_body = "\n".join(lines)
        if not clean_body or len(clean_body) < 30:
            return "ë³¸ë¬¸ ì—†ìŒ"
        return clean_body
    except Exception as e:
        print(f"[ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜]: {e}")
        return "ë³¸ë¬¸ ì—†ìŒ"

# ===================== [KoBART ìš”ì•½ - HF API] =====================
def summarize_kobart(text):
    for attempt in range(2):  # ìµœëŒ€ 2íšŒ ì¬ì‹œë„
        try:
            print("âœï¸ KoBART ìš”ì•½ ìš”ì²­")
            api_url = "https://api-inference.huggingface.co/models/digit82/kobart-summarization"
            headers = {
                "Authorization": f"Bearer {HF_API_TOKEN}",
                "Content-Type": "application/json"
            }

            payload = {
                "inputs": text[:1024],
                "options": {"wait_for_model": True}
            }

            response = requests.post(api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            if isinstance(result, list) and "summary_text" in result[0]:
                return result[0]["summary_text"]
            return "ìš”ì•½ ì‹¤íŒ¨"
        except Exception as e:
            print(f"âš ï¸ ìš”ì•½ ì¬ì‹œë„ {attempt + 1}íšŒ ì‹¤íŒ¨: {e}")
            time.sleep(1)
    return "ìš”ì•½ ì‹¤íŒ¨"

# ===================== [DB ì €ì¥] =====================
def save_to_sqlite(df, db_path=None, table_name="news", max_articles=150):
    base_dir = os.path.dirname(__file__)
    db_path = os.path.join(base_dir, "news_articles.db")
    print(f"[DB ì €ì¥ ê²½ë¡œ]: {db_path}")
    today = datetime.today().strftime("%Y%m%d")
    try:
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        cur.execute(f"SELECT COUNT(*) FROM {table_name}")
        current_count = cur.fetchone()[0]
        new_count = len(df)
        delete_count = max(0, (current_count + new_count) - max_articles)
        if delete_count > 0:
            cur.execute(f"DELETE FROM {table_name} WHERE rowid IN (SELECT rowid FROM {table_name} ORDER BY date, rowid LIMIT ?)", (delete_count,))
            print(f"ğŸ—‘ï¸ {delete_count}ê°œ ì˜¤ë˜ëœ ê¸°ì‚¬ ì‚­ì œ")
        for _, row in df.iterrows():
            try:
                cur.execute(f"""
                    INSERT INTO {table_name} (source, title, link, content, summary, date)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (row['source'], row['title'], row['link'], row['content'], row['summary'], today))
            except Exception as row_e:
                print(f"âŒ ê°œë³„ ì €ì¥ ì‹¤íŒ¨: {row['title']} | ì´ìœ : {row_e}")
        conn.commit()
        conn.close()
        print("[DB ì €ì¥ ì™„ë£Œ âœ…]")
    except Exception as e:
        print(f"[DB ì €ì¥ ì‹¤íŒ¨ âŒ]: {e}")

# ===================== [ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ ë½] =====================
run_lock = threading.Lock()

def run_news_job():
    if not run_lock.acquire(blocking=False):
        print("âš ï¸ ì´ë¯¸ ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ì§„í–‰ì¤‘ì…ë‹ˆë‹¤. ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€.")
        return
    try:
        print(f"\n[{datetime.now()}] ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        data = []
        per_feed = max(1, 150 // len(rss_feeds))
        for source, rss_url in rss_feeds.items():
            print(f"ğŸ“¡ [RSS ìš”ì²­] ì–¸ë¡ ì‚¬: {source} | URL: {rss_url}")
            feed = feedparser.parse(rss_url)
            print(f"âœ… [RSS ìˆ˜ì‹  ì™„ë£Œ] {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            for entry in feed.entries[:per_feed]:
                title = entry.title.strip().replace("\n", " ").replace(",", " ")
                link = entry.link
                print(f"ğŸ”— ê¸°ì‚¬ ì œëª©: {title}")
                print(f"ğŸ§­ ê¸°ì‚¬ ë§í¬: {link}")
                content = extract_body(link, source=source)
                print(f"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
                if content == "ë³¸ë¬¸ ì—†ìŒ":
                    print("âš ï¸ ë³¸ë¬¸ ì—†ìŒ - ìš”ì•½ ìƒëµ")
                    summary = "ìš”ì•½ ìƒëµ (ë³¸ë¬¸ ë¶€ì¡±)"
                else:
                    summary = summarize_kobart(content)
                    print(f"ğŸ“š ìš”ì•½ ë‚´ìš©: {summary[:50]}...")
                data.append({
                    "source": source,
                    "title": title,
                    "link": link,
                    "content": content,
                    "summary": summary
                })
                time.sleep(0.1)
        df = pd.DataFrame(data).drop_duplicates(subset="title")
        print(f"ğŸ’¾ ìµœì¢… ì €ì¥ ëŒ€ìƒ: {len(df)}ê±´ / ì›ë³¸: {len(data)}ê±´")
        if df.empty:
            print("âŒ ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
        else:
            print(f"âœ… DB ì €ì¥")
            save_to_sqlite(df)
        print(f"[{datetime.now()}] âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"[ğŸ”¥ ì˜ˆì™¸ ë°œìƒ] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    finally:
        run_lock.release()

# === í‚¤ìœ„ ê¸°ë°˜ ëª…ì‚¬ ì¶”ì¶œ & í‚¤ì›Œë“œ í•¨ìˆ˜ ===
def extract_nouns_kiwi(text):
    nouns = []
    for word, pos, _, _ in kiwi.analyze(text)[0][0]:
        if pos in ("NNG", "NNP"):
            nouns.append(word)
    return nouns

def extract_keywords_kiwi(texts, top_n=5):
    stopwords = set([
        "ë‰´ìŠ¤", "ê¸°ì", "í•œêµ­", "ì •ë¶€", "ì˜¤ëŠ˜", "ì œê³µ", "ê´€ë ¨", "ë³´ë„", "ì‚¬ì‹¤", "í†µí•´", "ìœ„í•´",
        "ë“±", "ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ëª…", "ì œ", "ì‹œ", "ë•Œ", "í›„", "ìœ„", "ì•", "ë’¤",
        "ì¤‘", "ë‚´", "ë°–", "ì´í›„", "ìœ„í•´", "ëŒ€í•´", "ëŒ€í•œ", "ì—", "ì™€", "ê³¼", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼",
        "ë¡œ", "ìœ¼ë¡œ", "ì—", "ì˜", "ì™€", "ê³¼", "ë„", "ê²ƒìœ¼ë¡œ", "ê°€ìš´ë°", "ëŒ€í†µë ¹ì€", "ë‚˜ëˆ”ì˜", "ëŒ€í†µë ¹ì´", "ë¬¼ë¡ ", "ë˜ê² ë‹¤", "ì—…ë¬´", "ë³´ê³ "
    ])
    all_nouns = []
    for text in texts:
        nouns = extract_nouns_kiwi(text)
        all_nouns.extend([n for n in nouns if n not in stopwords and len(n) > 1])
    from collections import Counter
    counter = Counter(all_nouns)
    return [{"keyword": w, "count": c} for w, c in counter.most_common(top_n)]

# ===================== [FastAPI ì—”ë“œí¬ì¸íŠ¸] =====================
@app.get("/run-news")
def run_news_direct(background_tasks: BackgroundTasks):
    background_tasks.add_task(run_news_job)
    return {"message": "ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘í–ˆìŠµë‹ˆë‹¤."}

@app.get("/trending-keywords")
def get_trending_keywords():
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT title, summary FROM news", conn)
        conn.close()
        recent = df.tail(150)
        if len(recent) < 50:
            sampled = recent
        else:
            sampled = recent.sample(n=50, random_state=None)
        combined = (sampled["title"].fillna("") + " " + sampled["summary"].fillna(""))
        keywords = extract_keywords_kiwi(combined.tolist(), top_n=5)
        return {"keywords": keywords}
    except Exception as e:
        print(f"âŒ trending-keywords ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

@app.get("/search-articles")
def search_articles(keyword: str = Query(..., min_length=2)):
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT source, title, summary, content, link FROM news", conn)
        conn.close()
        df = df.fillna("")
        filtered = df[
            df["title"].str.contains(keyword, case=False, na=False) |
            df["summary"].str.contains(keyword, case=False, na=False)
        ].copy()
        filtered["row_order"] = filtered.index[::-1]
        latest_by_source = filtered.sort_values("row_order").drop_duplicates("source")
        articles = latest_by_source.sort_values("row_order").head(3)
        return {
            "keyword": keyword,
            "articles": articles[["title", "summary", "content", "source", "link"]].to_dict(orient="records")
        }
    except Exception as e:
        print(f"âŒ search-articles ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

class SummaryRequest(BaseModel):
    keyword: str
    contents: List[str]

@app.post("/summarize-conclusion")
def summarize_conclusion(data: SummaryRequest):
    keyword = data.keyword
    context = "\n".join(data.contents[:3])
    prompt = f"""
    ë‹¹ì‹ ì€ ë‹¤ìˆ˜ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë¶„ì„í•´ ë³¸ì§ˆì  ìŸì ì„ ë„ì¶œí•˜ëŠ” ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ì•„ë˜ëŠ” '{keyword}'ì™€ ê´€ë ¨ëœ ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ì˜ ê¸°ì‚¬ ì›ë¬¸ì…ë‹ˆë‹¤.
    ê¸°ì‚¬ì˜ ì¤‘ë³µ í‘œí˜„ì´ë‚˜ ë‹¨ìˆœ ì‚¬ì‹¤ ë‚˜ì—´ì´ ì•„ë‹Œ, **í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ** ì •ë¦¬í•´ ì£¼ì„¸ìš”.

    ë‹¤ìŒ 3ê°€ì§€ í•­ëª©ì„ ê¸°ì¤€ìœ¼ë¡œ, **ê° í•­ëª©ì„ ë°˜ë“œì‹œ 1ë¬¸ì¥**ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.

    1. "fact":  
        - ì—¬ëŸ¬ ê¸°ì‚¬ì— ë°˜ë³µì ìœ¼ë¡œ ë“±ì¥í•˜ëŠ” **ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ ì‚¬ì‹¤**ì„ ëª…í™•í•˜ê²Œ,  
        - **íŒ©íŠ¸(ê°ê´€ì  ì§„ìˆ )**ë§Œ ë‹´ì•„ ìš”ì•½

    2. "issue":  
        - í•´ë‹¹ ë‰´ìŠ¤ ì´ìŠˆì˜ **ì£¼ìš” ìŸì /ê°ˆë“±/ë…¼ë€/ì‚¬íšŒì  ë°˜í–¥**ì„ ìš”ì•½  
        - ì–¸ë¡ ì‚¬ë“¤ì˜ ê³µí†µì ìœ¼ë¡œ ì£¼ëª©í•œ **ë¬¸ì œì ì´ë‚˜ ë…¼ìŸ**ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ì„œìˆ 

    3. "outlook":  
        - ì „ë¬¸ê°€ ë˜ëŠ” ì–¸ë¡ ë“¤ì´ ì œì‹œí•œ **ë¯¸ë˜ ì „ë§, ì˜í–¥, ì‹œì‚¬ì **ì„  
        - **ë¹„íŒì /í†µí•©ì  ê´€ì **ì—ì„œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬

    **í˜•ì‹**
    - ì•„ë˜ JSON ì˜ˆì‹œì²˜ëŸ¼, ê° í•­ëª© ì´ë¦„ì€ "fact", "issue", "outlook" ê·¸ëŒ€ë¡œ, ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€
    - ì§ì ‘ ì¸ìš© ì—†ì´ ë‹¹ì‹ ì˜ ì–¸ì–´ë¡œ ìš”ì•½, ì ˆëŒ€ ê¸°ì‚¬ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ë³µì‚¬ ê¸ˆì§€
    - ëª¨í˜¸í•˜ê±°ë‚˜ ê³¼ì¥ëœ í‘œí˜„, ì¶”ì¸¡ì€ ì§€ì–‘
    - ì˜ˆì¸¡/ì „ë§(outlook)ì€ ê¸°ì‚¬ ë‚´ì— ê·¼ê±°ê°€ ìˆì„ ë•Œë§Œ ì œì‹œ

    ì˜ˆì‹œ:
    {{
      "fact": "...",
      "issue": "...",
      "outlook": "..."
    }}

    [ê¸°ì‚¬ ì›ë¬¸ ëª¨ìŒ]
    {context}
    """.strip()

    try:
        from openai import OpenAI
        client = OpenAI()
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=500
        )
        result = response.choices[0].message.content.strip()
        return {"keyword": keyword, "summary": json.loads(result)}
    except Exception as e:
        print(f"âŒ summarize-conclusion ì˜¤ë¥˜: {e}")
        return {
            "keyword": keyword,
            "summary": {"fact": "ìš”ì•½ ì‹¤íŒ¨", "issue": "ìš”ì•½ ì‹¤íŒ¨", "outlook": "ìš”ì•½ ì‹¤íŒ¨"},
            "error": str(e)
        }

@app.get("/debug-news")
def debug_news():
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT source, title, summary, date FROM news ORDER BY date DESC LIMIT 10", conn)
        conn.close()
        return JSONResponse(content=df.to_dict(orient="records"))
    except Exception as e:
        print(f"âŒ debug-news ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

# ===================== [ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡] =====================
@app.on_event("startup")
def start_scheduler():
    ensure_db()
    scheduler = BackgroundScheduler()
    scheduler.add_job(run_news_job, "interval", hours=1)
    scheduler.start()
    print("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨: 1ì‹œê°„ë§ˆë‹¤ ë‰´ìŠ¤ ìˆ˜ì§‘")

if __name__ == "__main__":
    run_news_job()