from fastapi import FastAPI, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
from apscheduler.schedulers.background import BackgroundScheduler
from sklearn.feature_extraction.text import TfidfVectorizer
from dotenv import load_dotenv
import pandas as pd
import feedparser
import requests
import sqlite3
import time
import os
import json
import random
import re
from bs4 import BeautifulSoup
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import openai

# ===================== [ì´ˆê¸° ì„¤ì •] =====================
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

model_name = "digit82/kobart-summarization"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

rss_feeds = {
    "ì „ìì‹ ë¬¸": "https://rss.etnews.com/Section901.xml",
    "í•œê²¨ë ˆ": "https://www.hani.co.kr/rss/",
    "SBS": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01",
    "ë§¤ì¼ê²½ì œ": "https://www.mk.co.kr/rss/40300001/",
    "ì„¸ê³„ì¼ë³´": "https://www.segye.com/Articles/RSSList/segye_recent.xml"
}

# ===================== [DB ë³´ì¥ í•¨ìˆ˜] =====================
def ensure_db():
    db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS news (
            source TEXT,
            title TEXT,
            link TEXT,
            content TEXT,
            summary TEXT,
            date TEXT
        )
    """)
    conn.commit()
    conn.close()
    print("âœ… news í…Œì´ë¸” í™•ì¸ ë˜ëŠ” ìƒì„± ì™„ë£Œ")

# ===================== [í•µì‹¬ ê¸°ëŠ¥] =====================
def extract_body(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, timeout=5, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        paragraphs = soup.find_all("p")
        body = "\n".join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
        if not body or len(body) < 30 or any(kw in body.lower() for kw in ["ì‚­ì œ", "ì—†ìŒ", "404"]):
            return "ë³¸ë¬¸ ì—†ìŒ"
        return body
    except:
        return "ë³¸ë¬¸ ì—†ìŒ"

def summarize_kobart(text):
    try:
        if not text.strip():
            return "ìš”ì•½ ì—†ìŒ"
        inputs = tokenizer.encode(text[:1024], return_tensors="pt", truncation=True)
        summary_ids = model.generate(inputs, max_length=256, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)
        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    except:
        return "ìš”ì•½ ì‹¤íŒ¨"

def save_to_sqlite(df, db_path=None, table_name="news"):
    base_dir = os.path.dirname(__file__)
    db_path = os.path.join(base_dir, "news_articles.db")
    today = datetime.today().strftime("%Y%m%d")

    try:
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        for _, row in df.iterrows():
            cur.execute(f"""
                INSERT INTO {table_name} (source, title, link, content, summary, date)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (row['source'], row['title'], row['link'], row['content'], row['summary'], today))
        conn.commit()
        conn.close()
        print("[DB ì €ì¥ ì™„ë£Œ âœ…]")
    except Exception as e:
        print(f"[DB ì €ì¥ ì‹¤íŒ¨ âŒ]: {e}")

def run_news_job():
    try:
        print(f"\n[{datetime.now()}] ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

        data = []
        for source, rss_url in rss_feeds.items():
            print(f"ğŸ“¡ [RSS ìš”ì²­] ì–¸ë¡ ì‚¬: {source} | URL: {rss_url}")
            feed = feedparser.parse(rss_url)
            print(f"âœ… [RSS ìˆ˜ì‹  ì™„ë£Œ] {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

            for entry in feed.entries[:25]:
                title = entry.title.strip().replace("\n", " ").replace(",", " ")
                link = entry.link
                print(f"ğŸ”— ê¸°ì‚¬ ì œëª©: {title}")
                print(f"ğŸ§­ ê¸°ì‚¬ ë§í¬: {link}")

                content = extract_body(link)
                print(f"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(content)}")

                if content == "ë³¸ë¬¸ ì—†ìŒ":
                    print("âš ï¸ ë³¸ë¬¸ ì—†ìŒ - ìš”ì•½ ìƒëµ")
                    summary = "ìš”ì•½ ìƒëµ (ë³¸ë¬¸ ë¶€ì¡±)"
                else:
                    summary = summarize_kobart(content)
                    print(f"ğŸ“š ìš”ì•½ ë‚´ìš©: {summary[:50]}...")  # ìš”ì•½ ì• 50ìë§Œ ì¶œë ¥

                data.append({
                    "source": source,
                    "title": title,
                    "link": link,
                    "content": content,
                    "summary": summary
                })

                time.sleep(0.2)

        df = pd.DataFrame(data).drop_duplicates(subset="title")
        print(f"ğŸ“Š ëˆ„ì  ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(data)}")
        print(f"ğŸ“¦ ìµœì¢… ì €ì¥ ëŒ€ìƒ ë‰´ìŠ¤ ìˆ˜ (ì¤‘ë³µ ì œê±° í›„): {len(df)}")
        if df.empty:
            print("âŒ ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        else:
            print(f"âœ… DB ì €ì¥ ì‹œì‘ - ì˜ˆì‹œ ì œëª©: {df.iloc[0]['title']}")
        save_to_sqlite(df)

        print(f"[{datetime.now()}] âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        print(f"[ğŸ”¥ ì˜ˆì™¸ ë°œìƒ] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

def extract_keywords(texts, top_n=5):
    stopwords = set(["ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë˜í•œ", "ë“±", "ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ìœ¼ë¡œ", "ë“¤", "ì—ì„œ", "í•˜ë‹¤", "í•œ", "ëŒ€í•´",
        "ìˆë‹¤", "ê²€ìƒ‰í•´ì¤˜", "ì„¸ê³„ë¡œ", "ì´ëŒì–´ì¤„", "ëŒ€í•œ","ë°í˜”ë‹¤", "í›„ë³´ëŠ”", "ì¹ í•œ", "ì§€ë‚œ", "ìˆëŠ”", "ì£¼ìš”", "ë¡œ", "ì€", "ëŠ”", "ì´", "ê°€",
        "ì„", "ë¥¼", "ì—", "ì˜", "ì™€", "ê³¼", "ë„", "ê²ƒìœ¼ë¡œ", "ê°€ìš´ë°", "ëŒ€í†µë ¹ì€", "ë‚˜ëˆ”ì˜", "ëŒ€í†µë ¹ì´", "ë¬¼ë¡ ", "ë˜ê² ë‹¤",
        "ë§Œì—", "ë‚´ì¼", "ë‹¹ì‹ ì˜", "ê¸°ì‚¬ë¥¼", "ë™í–¥ê³¼", "ì •ë¶€ì˜","ë°”íƒ•ìœ¼ë¡œ", "ìœ„í•œ", "ìœ„í•´", "ì •ë¶€ëŠ”", "ì¥ê±°ë¦¬ìš”","ê°œìµœí–ˆë‹¤","ìµœê·¼","íœ˜ë‘ë¥¸","ë§ˆì¹˜ê³ "
        ,"ë¹ ë¥¸","í†µí•´","ë¬´ìŠ¨","ìˆìœ¼ë©°","ê¸°ìˆ ì„","íƒí—˜ê³¼","ì‹œì¥ì˜","ìˆ¨ì ¸","ë„¤ì´ë²„ì˜","ëª…ì˜","ê²ƒì´","ì§€ì›ì„","ì»¤ì§€ê³ ","ê¸°ì—…ì˜","ì”¨ê°€","ë¬¸ì œë¥¼","ì´í›„",
        "íƒ€ë”ë‹ˆ","ì „ë§ë„","ë“±ì„","ë°›ê³ ","ê¸°ì—…ë“¤ì˜","ì—†ëŠ”","ê°€ëŠ¥ì„±ê¹Œì§€","ê°œì—","ë‹¤ì‹œ","ê²€ìƒ‰í•´ì¤˜ìµœê·¼","ë§ˆë¦¬","ëª¨ë‘","ìˆë‹¤ê³ ","ì•Œë¦¼ì„","í•¨ê»˜","ë‚´ìš©ì„",
        "ê°œë°©í•˜ëŠ”","ìš°ë¦¬ê°€","ì—´ë¦°","ê²ƒì„","ê´€ë ¨","ë“±ë¡ì¼ì","ë¼ëŠ”","ê´€ì‹¬","ì¶”ê°€", "ê´€ì‹¬", "í™œìš©í•´","ì§€ì ","ë”°ë¥´ë©´","ê°•í•œ","ë§ˆì¹œ","ë‚˜ì˜¤ê³ ","ë°©ì•ˆì„",
        "ì¤‘ìš”í•˜ë‹¤ê³ ","ì˜ì§€ë¥¼","êµ¬ì²´ì ì¸","ë…¼ë€ë·°í‹°ë‹¹ì‹ ì˜","ê¸°ì‚¬","ì”¨ë¥¼","ê¸°ë°˜","ë§ì•„","íƒì‹œì—ì„œ","í¬ê²Œ","ê°•ë ¥","ì‚¬ë¡€ê°€","ë§¤ê²½","ë§¥ë½ì„","ë°œí‘œëœ","ìœ„ë¥¼",
        "ê°ê°","ìµœì‹ ","ì´ë¥¼","ë°ì´í„°ë¥¼","êµ­ë¯¼ì—ê²Œ","ë°œê²¬","ìƒìŠ¹ë¥ ì€"
                     ])
    try:
        tokenized = []
        for text in texts:
            words = re.findall(r"[ê°€-í£]{2,}", text)
            words = [w for w in words if w not in stopwords]
            tokenized.append(" ".join(words))
        if not any(tokenized):
            return []
        vectorizer = TfidfVectorizer()
        X = vectorizer.fit_transform(tokenized)
        feature_names = vectorizer.get_feature_names_out()
        word_counts = (X > 0).sum(axis=0).A1
        keywords = [(feature_names[i], int(word_counts[i])) for i in range(len(feature_names))]
        keywords.sort(key=lambda x: x[1], reverse=True)
        return [{"keyword": w, "count": c} for w, c in keywords[:top_n]]
    except:
        return []

# ===================== [FastAPI ì—”ë“œí¬ì¸íŠ¸] =====================
@app.get("/run-news")
def trigger_run(background_tasks: BackgroundTasks):
    background_tasks.add_task(run_news_job)
    return {"message": "ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤."}

@app.get("/trending-keywords")
def get_trending_keywords():
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT title, summary FROM news", conn)
        conn.close()
        combined = (df["title"].fillna("") + " " + df["summary"].fillna(""))
        sampled = random.sample(combined.tolist(), min(40, len(combined)))
        keywords = extract_keywords(sampled, top_n=5)
        return {"keywords": keywords}
    except Exception as e:
        return {"error": str(e)}

@app.get("/search-articles")
def search_articles(keyword: str = Query(..., min_length=2)):
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT source, title, summary, content, link FROM news", conn)
        conn.close()
        df = df.fillna("")
        filtered = df[
            df["title"].str.contains(keyword, case=False, na=False) |
            df["summary"].str.contains(keyword, case=False, na=False)
        ].copy()
        filtered["row_order"] = filtered.index[::-1]
        latest_by_source = filtered.sort_values("row_order").drop_duplicates("source")
        articles = latest_by_source.sort_values("row_order").head(3)
        return {
            "keyword": keyword,
            "articles": articles[["title", "summary", "content", "source", "link"]].to_dict(orient="records")
        }
    except Exception as e:
        return {"error": str(e)}

class SummaryRequest(BaseModel):
    keyword: str
    contents: List[str]

@app.post("/summarize-conclusion")
def summarize_conclusion(data: SummaryRequest):
    keyword = data.keyword
    context = "\n".join(data.contents[:3])
    prompt = f"""
    ë‹¤ìŒì€ '{keyword}'ì— ëŒ€í•œ ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ì˜ ê¸°ì‚¬ ì›ë¬¸ì…ë‹ˆë‹¤.

    ì´ ê¸°ì‚¬ë“¤ì˜ ê³µí†µëœ ì£¼ì œë¥¼ ë‹¤ìŒ 3ê°€ì§€ í•­ëª©ìœ¼ë¡œ ê°„ê²°íˆ ì •ë¦¬í•´ì¤˜.

    ìš”ì•½ í˜•ì‹ì€ ë‹¤ìŒ JSON í˜•íƒœ ê·¸ëŒ€ë¡œ ì¶œë ¥í•´ì¤˜:
    {{
      "fact": "í•µì‹¬ ì‚¬ì‹¤ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
      "issue": "ì‹ ë¬¸ì‚¬ë“¤ì˜ ê³µí†µëœ ìŸì ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
      "outlook": "í–¥í›„ ì „ë§ ë˜ëŠ” ì¢…í•© íŒë‹¨ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"
    }}

    ì¡°ê±´:
    - ê° í•­ëª©ì€ ë°˜ë“œì‹œ 1ë¬¸ì¥
    - ì§ì ‘ ì¸ìš© ì—†ì´ ìš”ì ì„ ëª…í™•íˆ ì„œìˆ 
    - í•­ëª© ì´ë¦„ì€ ë°˜ë“œì‹œ "fact", "issue", "outlook"ë§Œ ì‚¬ìš©
    - ë°˜ë“œì‹œ JSON í˜•ì‹ ìœ ì§€

    ê¸°ì‚¬ ì›ë¬¸:
    {context}
    """.strip()

    try:
        from openai import OpenAI
        client = OpenAI()
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=500
        )
        result = response.choices[0].message.content.strip()
        return {"keyword": keyword, "summary": json.loads(result)}
    except Exception as e:
        return {
            "keyword": keyword,
            "summary": {"fact": "ìš”ì•½ ì‹¤íŒ¨", "issue": "ìš”ì•½ ì‹¤íŒ¨", "outlook": "ìš”ì•½ ì‹¤íŒ¨"},
            "error": str(e)
        }

# ===================== [ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡] =====================
@app.on_event("startup")
def start_scheduler():
    ensure_db()
    scheduler = BackgroundScheduler()
    scheduler.add_job(run_news_job, "interval", hours=1)
    scheduler.start()
    print("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨: 1ì‹œê°„ë§ˆë‹¤ ë‰´ìŠ¤ ìˆ˜ì§‘")

from fastapi.responses import JSONResponse

@app.get("/debug-news")
def debug_news():
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT source, title, summary, date FROM news ORDER BY date DESC LIMIT 5", conn)
        conn.close()
        return JSONResponse(content=df.to_dict(orient="records"))
    except Exception as e:
        return {"error": str(e)}

if __name__ == "__main__":
    run_news_job()