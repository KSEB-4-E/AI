from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
from apscheduler.schedulers.background import BackgroundScheduler
from sklearn.feature_extraction.text import TfidfVectorizer
from dotenv import load_dotenv
import pandas as pd
import feedparser
import requests
import sqlite3
import time
import os
import json
import random
import re
from bs4 import BeautifulSoup
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import openai
from fastapi.responses import JSONResponse
from kiwipiepy import Kiwi

# ===================== [ì´ˆê¸° ì„¤ì •] =====================
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

model_name = "digit82/kobart-summarization"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

rss_feeds = {
    "ì „ìì‹ ë¬¸": "https://rss.etnews.com/Section901.xml",
    "í•œê²¨ë ˆ": "https://www.hani.co.kr/rss/",
    "SBS": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01",
    "ë§¤ì¼ê²½ì œ": "https://www.mk.co.kr/rss/40300001/",
    "ì„¸ê³„ì¼ë³´": "https://www.segye.com/Articles/RSSList/segye_recent.xml"
}

kiwi = Kiwi()

# ===================== [DB ë³´ì¥ í•¨ìˆ˜] =====================
def ensure_db():
    db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
    conn = sqlite3.connect(db_path)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS news (
            source TEXT,
            title TEXT,
            link TEXT,
            content TEXT,
            summary TEXT,
            date TEXT
        )
    """)
    conn.commit()
    conn.close()
    print("âœ… news í…Œì´ë¸” í™•ì¸ ë˜ëŠ” ìƒì„± ì™„ë£Œ")

# ===================== [í•µì‹¬ ê¸°ëŠ¥] =====================
def extract_body(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, timeout=5, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        paragraphs = soup.find_all("p")
        body = "\n".join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
        if not body or len(body) < 30 or any(kw in body.lower() for kw in ["ì‚­ì œ", "ì—†ìŒ", "404"]):
            return "ë³¸ë¬¸ ì—†ìŒ"
        return body
    except Exception as e:
        print(f"[ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜]: {e}")
        return "ë³¸ë¬¸ ì—†ìŒ"

def summarize_kobart(text):
    try:
        print("âœï¸ KoBART ìš”ì•½ ì‹œì‘")
        if not text.strip():
            return "ìš”ì•½ ì—†ìŒ"
        inputs = tokenizer.encode(text[:1024], return_tensors="pt", truncation=True)
        summary_ids = model.generate(inputs, max_length=256, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)
        result = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        print("âœ… ìš”ì•½ ì™„ë£Œ")
        return result
    except Exception as e:
        print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
        return "ìš”ì•½ ì‹¤íŒ¨"

def save_to_sqlite(df, db_path=None, table_name="news", max_articles=150):
    base_dir = os.path.dirname(__file__)
    db_path = os.path.join(base_dir, "news_articles.db")
    print(f"[DB ì €ì¥ ê²½ë¡œ]: {db_path}")
    today = datetime.today().strftime("%Y%m%d")
    try:
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        # í˜„ì¬ ì €ì¥ëœ ë‰´ìŠ¤ ê°œìˆ˜
        cur.execute(f"SELECT COUNT(*) FROM {table_name}")
        current_count = cur.fetchone()[0]
        new_count = len(df)
        delete_count = max(0, (current_count + new_count) - max_articles)
        if delete_count > 0:
            cur.execute(f"DELETE FROM {table_name} WHERE rowid IN (SELECT rowid FROM {table_name} ORDER BY date, rowid LIMIT ?)", (delete_count,))
            print(f"ğŸ—‘ï¸ {delete_count}ê°œ ì˜¤ë˜ëœ ê¸°ì‚¬ ì‚­ì œ")
        for _, row in df.iterrows():
            try:
                cur.execute(f"""
                    INSERT INTO {table_name} (source, title, link, content, summary, date)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (row['source'], row['title'], row['link'], row['content'], row['summary'], today))
            except Exception as row_e:
                print(f"âŒ ê°œë³„ ì €ì¥ ì‹¤íŒ¨: {row['title']} | ì´ìœ : {row_e}")
        conn.commit()
        conn.close()
        print("[DB ì €ì¥ ì™„ë£Œ âœ…]")
    except Exception as e:
        print(f"[DB ì €ì¥ ì‹¤íŒ¨ âŒ]: {e}")

def run_news_job():
    try:
        print(f"\n[{datetime.now()}] ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        data = []
        per_feed = max(1, 150 // len(rss_feeds))
        for source, rss_url in rss_feeds.items():
            print(f"ğŸ“¡ [RSS ìš”ì²­] ì–¸ë¡ ì‚¬: {source} | URL: {rss_url}")
            feed = feedparser.parse(rss_url)
            print(f"âœ… [RSS ìˆ˜ì‹  ì™„ë£Œ] {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            for entry in feed.entries[:per_feed]:
                title = entry.title.strip().replace("\n", " ").replace(",", " ")
                link = entry.link
                print(f"ğŸ”— ê¸°ì‚¬ ì œëª©: {title}")
                print(f"ğŸ§­ ê¸°ì‚¬ ë§í¬: {link}")
                content = extract_body(link)
                print(f"ğŸ“„ ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
                if content == "ë³¸ë¬¸ ì—†ìŒ":
                    print("âš ï¸ ë³¸ë¬¸ ì—†ìŒ - ìš”ì•½ ìƒëµ")
                    summary = "ìš”ì•½ ìƒëµ (ë³¸ë¬¸ ë¶€ì¡±)"
                else:
                    summary = summarize_kobart(content)
                    print(f"ğŸ“š ìš”ì•½ ë‚´ìš©: {summary[:50]}...")
                data.append({
                    "source": source,
                    "title": title,
                    "link": link,
                    "content": content,
                    "summary": summary
                })
                time.sleep(0.1)
        df = pd.DataFrame(data).drop_duplicates(subset="title")
        print(f"ğŸ’¾ ìµœì¢… ì €ì¥ ëŒ€ìƒ: {len(df)}ê±´ / ì›ë³¸: {len(data)}ê±´")
        if df.empty:
            print("âŒ ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
        else:
            print(f"âœ… DB ì €ì¥")
            save_to_sqlite(df)
        print(f"[{datetime.now()}] âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        print(f"[ğŸ”¥ ì˜ˆì™¸ ë°œìƒ] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

# === í‚¤ìœ„ ê¸°ë°˜ ëª…ì‚¬ ì¶”ì¶œ & í‚¤ì›Œë“œ í•¨ìˆ˜ ===
def extract_nouns_kiwi(text):
    nouns = []
    for word, pos, _, _ in kiwi.analyze(text)[0][0]:
        if pos in ("NNG", "NNP"):
            nouns.append(word)
    return nouns

def extract_keywords_kiwi(texts, top_n=5):
    stopwords = set([
        "ë‰´ìŠ¤", "ê¸°ì", "í•œêµ­", "ì •ë¶€", "ì˜¤ëŠ˜", "ì œê³µ", "ê´€ë ¨", "ë³´ë„", "ì‚¬ì‹¤", "í†µí•´", "ìœ„í•´",
        "ë“±", "ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜", "ëª…", "ì œ", "ì‹œ", "ë•Œ", "í›„", "ìœ„", "ì•", "ë’¤",
        "ì¤‘", "ë‚´", "ë°–", "ì´í›„", "ìœ„í•´", "ëŒ€í•´", "ëŒ€í•œ", "ì—", "ì™€", "ê³¼", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼",
        "ë¡œ", "ìœ¼ë¡œ", "ì—", "ì˜", "ì™€", "ê³¼", "ë„", "ê²ƒìœ¼ë¡œ", "ê°€ìš´ë°", "ëŒ€í†µë ¹ì€", "ë‚˜ëˆ”ì˜", "ëŒ€í†µë ¹ì´", "ë¬¼ë¡ ", "ë˜ê² ë‹¤",
        # ì¶”ê°€ í•„ìš”ì‹œ ê³„ì† ë³´ë©° ê´€ë¦¬!
    ])
    all_nouns = []
    for text in texts:
        nouns = extract_nouns_kiwi(text)
        all_nouns.extend([n for n in nouns if n not in stopwords and len(n) > 1])
    from collections import Counter
    counter = Counter(all_nouns)
    return [{"keyword": w, "count": c} for w, c in counter.most_common(top_n)]

# ===================== [FastAPI ì—”ë“œí¬ì¸íŠ¸] =====================
@app.get("/run-news")
def run_news_direct():
    try:
        run_news_job()
        return {"message": "ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì¦‰ì‹œ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."}
    except Exception as e:
        return {"error": str(e)}

@app.get("/trending-keywords")
def get_trending_keywords():
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT title, summary FROM news", conn)
        conn.close()
        recent = df.tail(150)
        sampled = random.sample(recent.index.tolist(), min(30, len(recent)))
        combined = (df.loc[sampled, "title"].fillna("") + " " + df.loc[sampled, "summary"].fillna(""))
        keywords = extract_keywords_kiwi(combined.tolist(), top_n=5)
        return {"keywords": keywords}
    except Exception as e:
        print(f"âŒ trending-keywords ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

@app.get("/search-articles")
def search_articles(keyword: str = Query(..., min_length=2)):
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT source, title, summary, content, link FROM news", conn)
        conn.close()
        df = df.fillna("")
        filtered = df[
            df["title"].str.contains(keyword, case=False, na=False) |
            df["summary"].str.contains(keyword, case=False, na=False)
        ].copy()
        filtered["row_order"] = filtered.index[::-1]
        latest_by_source = filtered.sort_values("row_order").drop_duplicates("source")
        articles = latest_by_source.sort_values("row_order").head(3)
        return {
            "keyword": keyword,
            "articles": articles[["title", "summary", "content", "source", "link"]].to_dict(orient="records")
        }
    except Exception as e:
        print(f"âŒ search-articles ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

class SummaryRequest(BaseModel):
    keyword: str
    contents: List[str]

@app.post("/summarize-conclusion")
def summarize_conclusion(data: SummaryRequest):
    keyword = data.keyword
    context = "\n".join(data.contents[:3])
    prompt = f"""
    ë‹¤ìŒì€ '{keyword}'ì— ëŒ€í•œ ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ì˜ ê¸°ì‚¬ ì›ë¬¸ì…ë‹ˆë‹¤.
    ì´ ê¸°ì‚¬ë“¤ì˜ ê³µí†µëœ ì£¼ì œë¥¼ ë‹¤ìŒ 3ê°€ì§€ í•­ëª©ìœ¼ë¡œ ê°„ê²°íˆ ì •ë¦¬í•´ì¤˜.
    ìš”ì•½ í˜•ì‹ì€ ë‹¤ìŒ JSON í˜•íƒœ ê·¸ëŒ€ë¡œ ì¶œë ¥í•´ì¤˜:
    {{
      "fact": "í•µì‹¬ ì‚¬ì‹¤ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
      "issue": "ì‹ ë¬¸ì‚¬ë“¤ì˜ ê³µí†µëœ ìŸì ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
      "outlook": "í–¥í›„ ì „ë§ ë˜ëŠ” ì¢…í•© íŒë‹¨ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"
    }}
    ì¡°ê±´:
    - ê° í•­ëª©ì€ ë°˜ë“œì‹œ 1ë¬¸ì¥
    - ì§ì ‘ ì¸ìš© ì—†ì´ ìš”ì ì„ ëª…í™•íˆ ì„œìˆ 
    - í•­ëª© ì´ë¦„ì€ ë°˜ë“œì‹œ "fact", "issue", "outlook"ë§Œ ì‚¬ìš©
    - ë°˜ë“œì‹œ JSON í˜•ì‹ ìœ ì§€
    ê¸°ì‚¬ ì›ë¬¸:
    {context}
    """.strip()

    try:
        from openai import OpenAI
        client = OpenAI()
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=500
        )
        result = response.choices[0].message.content.strip()
        return {"keyword": keyword, "summary": json.loads(result)}
    except Exception as e:
        print(f"âŒ summarize-conclusion ì˜¤ë¥˜: {e}")
        return {
            "keyword": keyword,
            "summary": {"fact": "ìš”ì•½ ì‹¤íŒ¨", "issue": "ìš”ì•½ ì‹¤íŒ¨", "outlook": "ìš”ì•½ ì‹¤íŒ¨"},
            "error": str(e)
        }

@app.get("/debug-news")
def debug_news():
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT source, title, summary, date FROM news ORDER BY date DESC LIMIT 10", conn)
        conn.close()
        return JSONResponse(content=df.to_dict(orient="records"))
    except Exception as e:
        print(f"âŒ debug-news ì˜¤ë¥˜: {e}")
        return {"error": str(e)}

# ===================== [ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡] =====================
@app.on_event("startup")
def start_scheduler():
    ensure_db()
    scheduler = BackgroundScheduler()
    scheduler.add_job(run_news_job, "interval", hours=1)
    scheduler.start()
    print("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨: 1ì‹œê°„ë§ˆë‹¤ ë‰´ìŠ¤ ìˆ˜ì§‘")

if __name__ == "__main__":
    run_news_job()