from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
from openai import OpenAI
from dotenv import load_dotenv
import mysql.connector
import pandas as pd
from kiwipiepy import Kiwi
from collections import Counter
import os
from typing import Optional

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
load_dotenv()

# âœ… GPT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# âœ… DBì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜
def fetch_news_from_db():
    conn = mysql.connector.connect(
        host=os.getenv("DB_HOST"),       # ì˜ˆ: '127.0.0.1'
        user=os.getenv("DB_USER"),       # ì˜ˆ: 'root'
        password=os.getenv("DB_PASSWORD"),
        database=os.getenv("DB_NAME")    # ì˜ˆ: 'newsdb'
    )
    cursor = conn.cursor()
    cursor.execute("SELECT title, summary, content FROM news")
    rows = cursor.fetchall()
    cursor.close()
    conn.close()
    return pd.DataFrame(rows, columns=["title", "summary", "content"])

# âœ… FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# âœ… ì „ì—­ ë°ì´í„°í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°
df = fetch_news_from_db()

@app.get("/api/search")
def search(q: Optional[str] = Query(None)):
    print("ê²€ìƒ‰ì–´:", q) # ì½˜ì†” í™•ì¸ìš© ë¡œê·¸
    if not q:
        return {"results": [], "message": "ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤."}

# ğŸ‘‰ ë‚˜ì¤‘ì— DB ì—°ë™í•˜ê±°ë‚˜ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ë„£ìœ¼ë©´ ë¨
    return {
        "results": [f"ğŸ” '{q}'ì— ëŒ€í•œ ê°€ì§œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.", "ì˜ˆì‹œ 1", "ì˜ˆì‹œ 2"],
        "count": 3
    }

# âœ… ì¶”ì²œ í‚¤ì›Œë“œ API
@app.get("/trending-keywords")
def get_trending_keywords():
    texts = (df["title"].fillna("") + " " + df["summary"].fillna("")).tolist()[:100]
    kiwi = Kiwi()
    all_keywords = []
    for text in texts:
        all_keywords += [token.form for token in kiwi.tokenize(text)
                         if token.tag in ["NNG", "NNP"] and len(token.form) > 1]
    most_common = Counter(all_keywords).most_common(10)
    return {"keywords": [kw for kw, _ in most_common]}

# âœ… ê¸°ì‚¬ ê²€ìƒ‰ API
@app.get("/search-articles")
def search_articles(keyword: str = Query(..., min_length=2)):
    filtered = df[
        df["title"].fillna("").str.contains(keyword, case=False, regex=False) |
        df["summary"].fillna("").str.contains(keyword, case=False, regex=False) |
        df["content"].fillna("").str.contains(keyword, case=False, regex=False)
    ]
    articles = filtered[["title", "summary", "content"]].dropna().head(5).to_dict(orient="records")
    return {"keyword": keyword, "articles": articles}

# âœ… ê²°ë¡  ìš”ì•½ ìš”ì²­ ëª¨ë¸
class SummaryRequest(BaseModel):
    keyword: str
    contents: List[str]

# âœ… ê²°ë¡  ìš”ì•½ API
@app.post("/summarize-conclusion")
def summarize_conclusion(data: SummaryRequest):
    keyword = data.keyword
    contents = data.contents[:3]
    prompt = f"""
ë‹¤ìŒì€ '{keyword}'ì— ëŒ€í•œ ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ì˜ ê¸°ì‚¬ ì›ë¬¸ì…ë‹ˆë‹¤.

ì´ ê¸°ì‚¬ë“¤ì˜ í•µì‹¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì „ì²´ ì´ìŠˆì˜ íë¦„ì„ 3ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´ì¤˜.

- ìš”ì•½ì€ ì „ì²´ ê¸°ì‚¬ ë‚´ìš©ì˜ ì•½ 5~10% ì •ë„ ì••ì¶•ëœ ë¶„ëŸ‰ì´ê¸¸ ë°”ë¼.
- ê°ê°ì˜ ë¬¸ì¥ì€ (1) í•µì‹¬ ì‚¬ì‹¤, (2) ì–¸ë¡  ì…ì¥ ìš”ì•½, (3) í–¥í›„ ì „ë§ ë˜ëŠ” ì¢…í•© íŒë‹¨ ì„ ë‹´ë˜, ì¤‘ë¦½ì ì¸ ì‹œì„ ìœ¼ë¡œ ì¬êµ¬ì„±í•´ì¤˜.
- ì§ì ‘ì ì¸ ì¸ìš©ë³´ë‹¤ëŠ” ê°œë…ì„ ì •ë¦¬í•´ì„œ ì„œìˆ í•´ì¤˜.

""" + "\n\n".join([f"ê¸°ì‚¬ {i + 1}:\n{c}" for i, c in enumerate(contents)])

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=500
    )
    return {
        "keyword": keyword,
        "summary": response.choices[0].message.content.strip()
    }