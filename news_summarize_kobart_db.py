from fastapi import FastAPI, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
from apscheduler.schedulers.background import BackgroundScheduler
from sklearn.feature_extraction.text import TfidfVectorizer
from dotenv import load_dotenv
import pandas as pd
import feedparser
import requests
import sqlite3
import time
import os
import json
import random
import re
from bs4 import BeautifulSoup
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import openai

# ===================== [ì´ˆê¸° ì„¤ì •] =====================
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

model_name = "digit82/kobart-summarization"
tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir="/tmp")
model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir="/tmp")

rss_feeds = {
    "ì „ìì‹ ë¬¸": "https://rss.etnews.com/Section901.xml",
    "í•œê²¨ë ˆ": "https://www.hani.co.kr/rss/",
    "SBS": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01",
    "ë§¤ì¼ê²½ì œ": "https://www.mk.co.kr/rss/40300001/",
    "ì„¸ê³„ì¼ë³´": "https://www.segye.com/Articles/RSSList/segye_recent.xml"
}

# ===================== [í•µì‹¬ ê¸°ëŠ¥] =====================
def extract_body(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        res = requests.get(url, timeout=5, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        paragraphs = soup.find_all("p")
        body = "\n".join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
        if not body or len(body) < 30 or any(kw in body.lower() for kw in ["ì‚­ì œ", "ì—†ìŒ", "404"]):
            return "ë³¸ë¬¸ ì—†ìŒ"
        return body
    except Exception as e:
        print(f"[ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜]: {e}")
        return "ë³¸ë¬¸ ì—†ìŒ"

def summarize_kobart(text):
    try:
        if not text.strip():
            return "ìš”ì•½ ì—†ìŒ"
        inputs = tokenizer.encode(text[:1024], return_tensors="pt", truncation=True)
        summary_ids = model.generate(inputs, max_length=256, min_length=20, length_penalty=2.0, num_beams=4, early_stopping=True)
        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    except Exception as e:
        print(f"[ìš”ì•½ ì‹¤íŒ¨]: {e}")
        return "ìš”ì•½ ì‹¤íŒ¨"

def save_to_sqlite(df, db_path=None, table_name="news"):
    base_dir = os.path.dirname(__file__)
    db_path = os.path.join(base_dir, "news_articles.db")
    today = datetime.today().strftime("%Y%m%d")
    try:
        conn = sqlite3.connect(db_path)
        cur = conn.cursor()
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS {table_name} (
                source TEXT,
                title TEXT,
                link TEXT,
                content TEXT,
                summary TEXT,
                date TEXT
            )
        """)
        cur.execute(f"DELETE FROM {table_name} WHERE date < ?", (today,))
        for _, row in df.iterrows():
            cur.execute(f"""
                INSERT INTO {table_name} (source, title, link, content, summary, date)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (row['source'], row['title'], row['link'], row['content'], row['summary'], today))
        conn.commit()
        conn.close()
        print("[DB ì €ì¥ ì™„ë£Œ âœ…]")
    except Exception as e:
        print(f"[DB ì €ì¥ ì‹¤íŒ¨ âŒ]: {e}")

def run_news_job():
    try:
        print(f"[{datetime.now()}] ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        data = []
        for source, rss_url in rss_feeds.items():
            feed = feedparser.parse(rss_url)
            if not feed.entries:
                print(f"âš ï¸ í”¼ë“œ ì—†ìŒ: {source} - {rss_url}")
                continue
            for entry in feed.entries[:25]:
                try:
                    title = entry.title.strip().replace("\n", " ").replace(",", " ")
                    link = entry.link
                    content = extract_body(link)
                    summary = "ìš”ì•½ ìƒëµ (ë³¸ë¬¸ ë¶€ì¡±)" if content == "ë³¸ë¬¸ ì—†ìŒ" else summarize_kobart(content)
                    data.append({
                        "source": source, "title": title, "link": link,
                        "content": content, "summary": summary
                    })
                    print(f"ğŸ“Œ ê¸°ì‚¬ ìˆ˜ì§‘: {title}")
                    time.sleep(0.2)
                except Exception as e:
                    print(f"[ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜]: {e}")
        if data:
            df = pd.DataFrame(data).drop_duplicates(subset="title")
            save_to_sqlite(df)
            print(f"[{datetime.now()}] âœ… ë‰´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        else:
            print("âŒ ì €ì¥í•  ë‰´ìŠ¤ ì—†ìŒ")
    except Exception as e:
        print(f"[ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ âŒ]: {e}")

def extract_keywords(texts, top_n=5):
    stopwords = set([
        "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ë˜í•œ", "ë“±", "ì´", "ê·¸", "ì €", "ê²ƒ", "ìˆ˜",
        "ëª…ì´", "ìœ¼ë¡œ", "ëª…ìœ¼ë¡œ", "ë“¤", "ì—ì„œ", "í•˜ë‹¤", "í•œ", "ëŒ€í•´", "ìˆë‹¤", "ëŒ€í•œ",
        "ë°í˜”ë‹¤", "í›„ë³´ëŠ”", "ì¹ í•œ", "ì§€ë‚œ", "ìˆëŠ”", "ì£¼ìš”", "ë¡œ", "ì€", "ëŠ”", "ì´", "ê°€",
        "ì„", "ë¥¼", "ì—", "ì˜", "ì™€", "ê³¼", "ë„", "ê²ƒìœ¼ë¡œ", "ê°€ìš´ë°", "ëŒ€í†µë ¹ì€", "ë‚˜ëˆ”ì˜", "ëŒ€í†µë ¹ì´", "ë¬¼ë¡ ", "ë˜ê² ë‹¤",
        "ë§Œì—", "ë‚´ì¼", "ë‹¹ì‹ ì˜", "ê¸°ì‚¬ë¥¼", "ë™í–¥ê³¼", "ì •ë¶€ì˜"
    ])
    try:
        tokenized = []
        for text in texts:
            words = re.findall(r"[ê°€-í£]{2,}", text)
            words = [w for w in words if w not in stopwords]
            tokenized.append(" ".join(words))
        if not any(tokenized):
            return []
        vectorizer = TfidfVectorizer()
        X = vectorizer.fit_transform(tokenized)
        feature_names = vectorizer.get_feature_names_out()
        word_counts = (X > 0).sum(axis=0).A1
        keywords = [(feature_names[i], int(word_counts[i])) for i in range(len(feature_names))]
        keywords.sort(key=lambda x: x[1], reverse=True)
        return [{"keyword": w, "count": c} for w, c in keywords[:top_n]]
    except:
        return []

# ===================== [FastAPI ì—”ë“œí¬ì¸íŠ¸] =====================
@app.get("/run-news")
def trigger_run(background_tasks: BackgroundTasks):
    background_tasks.add_task(run_news_job)
    return {"message": "ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í–ˆìŠµë‹ˆë‹¤."}

@app.get("/trending-keywords")
def get_trending_keywords():
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT title, summary FROM news", conn)
        conn.close()
        combined = (df["title"].fillna("") + " " + df["summary"].fillna(""))
        sampled = random.sample(combined.tolist(), min(30, len(combined)))
        keywords = extract_keywords(sampled, top_n=5)
        return {"keywords": keywords}
    except Exception as e:
        return {"error": str(e)}

@app.get("/search-articles")
def search_articles(keyword: str = Query(..., min_length=2)):
    try:
        db_path = os.path.join(os.path.dirname(__file__), "news_articles.db")
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query("SELECT source, title, summary, content, link FROM news", conn)
        conn.close()
        df = df.fillna("")
        filtered = df[
            df["title"].str.contains(keyword, case=False, na=False) |
            df["summary"].str.contains(keyword, case=False, na=False)
        ].copy()
        filtered["row_order"] = filtered.index[::-1]
        latest_by_source = filtered.sort_values("row_order").drop_duplicates("source")
        articles = latest_by_source.sort_values("row_order").head(3)
        return {
            "keyword": keyword,
            "articles": articles[["title", "summary", "content", "source", "link"]].to_dict(orient="records")
        }
    except Exception as e:
        return {"error": str(e)}

class SummaryRequest(BaseModel):
    keyword: str
    contents: List[str]

@app.post("/summarize-conclusion")
def summarize_conclusion(data: SummaryRequest):
    keyword = data.keyword
    context = "\n".join(data.contents[:3])

    prompt = f"""
    ë‹¤ìŒì€ '{keyword}'ì— ëŒ€í•œ ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ì˜ ê¸°ì‚¬ ì›ë¬¸ì…ë‹ˆë‹¤.

    ì´ ê¸°ì‚¬ë“¤ì˜ ê³µí†µëœ ì£¼ì œë¥¼ ë‹¤ìŒ 3ê°€ì§€ í•­ëª©ìœ¼ë¡œ ê°„ê²°íˆ ì •ë¦¬í•´ì¤˜.

    ìš”ì•½ í˜•ì‹ì€ ë‹¤ìŒ JSON í˜•íƒœ ê·¸ëŒ€ë¡œ ì¶œë ¥í•´ì¤˜:
    {{
      "fact": "í•µì‹¬ ì‚¬ì‹¤ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
      "issue": "ì‹ ë¬¸ì‚¬ë“¤ì˜ ê³µí†µëœ ìŸì ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
      "outlook": "í–¥í›„ ì „ë§ ë˜ëŠ” ì¢…í•© íŒë‹¨ì„ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"
    }}

    ì¡°ê±´:
    - ê° í•­ëª©ì€ ë°˜ë“œì‹œ 1ë¬¸ì¥
    - ì§ì ‘ ì¸ìš© ì—†ì´ ìš”ì ì„ ëª…í™•íˆ ì„œìˆ 
    - í•­ëª© ì´ë¦„ì€ ë°˜ë“œì‹œ "fact", "issue", "outlook"ë§Œ ì‚¬ìš©
    - ë°˜ë“œì‹œ JSON í˜•ì‹ ìœ ì§€

    ê¸°ì‚¬ ì›ë¬¸:
    {context}
    """.strip()

    try:
        from openai import OpenAI
        client = OpenAI()
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=500
        )
        result = response.choices[0].message.content.strip()
        return {"keyword": keyword, "summary": json.loads(result)}
    except Exception as e:
        return {
            "keyword": keyword,
            "summary": {"fact": "ìš”ì•½ ì‹¤íŒ¨", "issue": "ìš”ì•½ ì‹¤íŒ¨", "outlook": "ìš”ì•½ ì‹¤íŒ¨"},
            "error": str(e)
        }

# ===================== [ìŠ¤ì¼€ì¤„ëŸ¬ ë“±ë¡] =====================
@app.on_event("startup")
def start_scheduler():
    scheduler = BackgroundScheduler()
    scheduler.add_job(run_news_job, "interval", hours=1)
    scheduler.start()
    print("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨: 1ì‹œê°„ë§ˆë‹¤ ë‰´ìŠ¤ ìˆ˜ì§‘")

# ===================== [ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ìš©] =====================
if __name__ == "__main__":
    run_news_job()